{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('features.csv', index_col='match_id')\n",
    "test = pd.read_csv('features_test.csv', index_col='match_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Удаление лишних признаков-----------------------\n",
    "\n",
    "y = data['radiant_win']    # Целевая переменная\n",
    "data.drop('radiant_win', axis=1, inplace=True)\n",
    "not_in_test = [col for col in data.columns if col not in test.columns]\n",
    "data.drop(not_in_test, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Обработка пропущенных данных-------------------\n",
    "\n",
    "merged_data = pd.concat([data, test])\n",
    "cols_with_nan = merged_data.loc[:, merged_data.isnull().any()].columns\n",
    "print(cols_with_nan)    # Признаки с пропущенными значениями\n",
    "\n",
    "data['first_blood_team'].fillna(-1, inplace=True)\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "test['first_blood_team'].fillna(-1, inplace=True)\n",
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Количество идентификаторов героев---------------------------\n",
    "\n",
    "heroes = 0\n",
    "for col in ['r1_hero','r2_hero','r3_hero','r4_hero','r5_hero','d1_hero','d2_hero','d3_hero','d4_hero','d5_hero']:\n",
    "    if merged_data[col].unique().max() > heroes:\n",
    "        heroes = merged_data[col].unique().max()\n",
    "heroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Построение классификатора------------------------\n",
    "\n",
    "#--------------------Градиентный бустинг------------------------\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "for k in [10, 20, 30, 40, 50]:\n",
    "    print(k, 'trees')\n",
    "    cls = GradientBoostingClassifier(n_estimators=k)\n",
    "    start_time = datetime.datetime.now()\n",
    "    cross_scores = cross_val_score(cls, data, y, cv=cv, scoring='roc_auc')\n",
    "    print('Time elapsed:', datetime.datetime.now() - start_time)\n",
    "    print('Mean score', np.around(cross_scores.mean(), decimals=2))\n",
    "    print('Min score', np.around(cross_scores.min(), decimals=2))\n",
    "    print('Max score {}\\n'.format(np.around(cross_scores.max(), decimals=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Логистическая регрессия------------------------\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "for c in np.power(10.0, np.arange(-5, 6)):\n",
    "    print('Regularization parameter', c)\n",
    "    cls = LogisticRegression(C=c)\n",
    "    start_time = datetime.datetime.now()\n",
    "    cross_scores = cross_val_score(cls, scaled_data, y, cv=cv, scoring='roc_auc')\n",
    "    print('Time elapsed:', datetime.datetime.now() - start_time)\n",
    "    print('Mean score', cross_scores.mean())\n",
    "    print('Min score', np.around(cross_scores.min(), decimals=2))\n",
    "    print('Max score {}\\n'.format(np.around(cross_scores.max(), decimals=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['first_blood_time', 'first_blood_team', 'first_blood_player1',\n",
      "       'first_blood_player2', 'radiant_bottle_time', 'radiant_courier_time',\n",
      "       'radiant_flying_courier_time', 'radiant_first_ward_time',\n",
      "       'dire_bottle_time', 'dire_courier_time', 'dire_flying_courier_time',\n",
      "       'dire_first_ward_time'],\n",
      "      dtype='object')\n",
      "112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min predicton 0.009\n",
      "Max predicton 0.997\n"
     ]
    }
   ],
   "source": [
    "#--------------------Логистическая регрессия после обработки------------------------\n",
    "\n",
    "X_pick = np.zeros((merged_data.shape[0], heroes))\n",
    "for i, match_id in enumerate(merged_data.index):\n",
    "    for p in range(5):\n",
    "        X_pick[i, merged_data.loc[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "        X_pick[i, merged_data.loc[match_id, 'd%d_hero' % (p+1)]-1] = -1\n",
    "\n",
    "data.drop(['lobby_type','r1_hero','r2_hero','r3_hero','r4_hero','r5_hero','d1_hero','d2_hero','d3_hero','d4_hero','d5_hero'], axis=1, inplace=True)\n",
    "test.drop(['lobby_type','r1_hero','r2_hero','r3_hero','r4_hero','r5_hero','d1_hero','d2_hero','d3_hero','d4_hero','d5_hero'], axis=1, inplace=True)\n",
    "\n",
    "scaled_data = np.hstack((scaled_data, X_pick[:data.shape[0]]))\n",
    "\n",
    "scaled_test = scaler.transform(test)\n",
    "scaled_test = np.hstack((scaled_test, X_pick[data.shape[0]:]))\n",
    "\n",
    "for c in np.power(10.0, np.arange(-5, 6)):\n",
    "    print('Regularization parameter', c)\n",
    "    cls = LogisticRegression(C=c)\n",
    "    start_time = datetime.datetime.now()\n",
    "    cross_scores = cross_val_score(cls, scaled_data, y, cv=cv, scoring='roc_auc')\n",
    "    print('Time elapsed:', datetime.datetime.now() - start_time)\n",
    "    print('Mean score', cross_scores.mean())\n",
    "    print('Min score', np.around(cross_scores.min(), decimals=2))\n",
    "    print('Max score {}\\n'.format(np.around(cross_scores.max(), decimals=2)))\n",
    "\n",
    "cls = LogisticRegression(C=10)\n",
    "cls.fit(scaled_data, y)\n",
    "pred = cls.predict_proba(scaled_test)[:, 1]\n",
    "print('Min predicton', np.around(pred.min(), decimals=3))\n",
    "print('Max predicton', np.around(pred.max(), decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
